{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"5xh9ZZMyT-8P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713897776414,"user_tz":-360,"elapsed":101253,"user":{"displayName":"Research73","userId":"16443954781959923532"}},"outputId":"cc474e46-46aa-47a4-9392-b076f1c704f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->thop)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->thop)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->thop)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->thop)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->thop)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->thop)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->thop)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->thop)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->thop)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238\n","Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n","Collecting timm\n","  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.4.127)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Installing collected packages: timm\n","Successfully installed timm-0.9.16\n"]}],"source":["!pip install thop\n","!pip install einops\n","!pip install timm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"kRpS4r39G1_j","executionInfo":{"status":"ok","timestamp":1713897787064,"user_tz":-360,"elapsed":10660,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import pandas as pd\n","from torchvision.transforms import ToTensor\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from torchvision import transforms\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from thop import profile\n","from einops import rearrange\n","from sklearn.metrics import accuracy_score, average_precision_score\n","from einops.layers.torch import Rearrange, Reduce\n","from timm.models.layers import trunc_normal_, DropPath"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25070,"status":"ok","timestamp":1713897812128,"user":{"displayName":"Research73","userId":"16443954781959923532"},"user_tz":-360},"id":"8dwlmhBD-RQl","outputId":"83f64807-fe77-4107-c432-1f566c13a8f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hQX_4XetG1_m","executionInfo":{"status":"ok","timestamp":1713897812128,"user_tz":-360,"elapsed":4,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["class WMSA(nn.Module):\n","    \"\"\" Self-attention module in Swin Transformer\n","    \"\"\"\n","\n","    def __init__(self, input_dim, output_dim, head_dim, window_size, type):\n","        super(WMSA, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.head_dim = head_dim\n","        self.scale = self.head_dim ** -0.5\n","        self.n_heads = input_dim//head_dim\n","        self.window_size = window_size\n","        self.type=type\n","        self.embedding_layer = nn.Linear(self.input_dim, 3*self.input_dim, bias=True)\n","\n","        #todo recover\n","        # self.relative_position_params = nn.Parameter(torch.zeros(self.n_heads, 2 * window_size - 1, 2 * window_size -1))\n","        self.relative_position_params = nn.Parameter(torch.zeros((2 * window_size - 1)*(2 * window_size -1), self.n_heads))\n","\n","        self.linear = nn.Linear(self.input_dim, self.output_dim)\n","\n","        trunc_normal_(self.relative_position_params, std=.02)\n","        self.relative_position_params = torch.nn.Parameter(self.relative_position_params.view(2*window_size-1, 2*window_size-1, self.n_heads).transpose(1,2).transpose(0,1))\n","\n","    def generate_mask(self, w, p, shift):\n","        \"\"\" generating the mask of SW-MSA\n","        Args:\n","            shift: shift parameters in CyclicShift.\n","        Returns:\n","            attn_mask: should be (1 1 w p p),\n","        \"\"\"\n","        # supporting sqaure.\n","        attn_mask = torch.zeros(w, w, p, p, p, p, dtype=torch.bool, device=self.relative_position_params.device)\n","        if self.type == 'W':\n","            return attn_mask\n","\n","        s = p - shift\n","        attn_mask[-1, :, :s, :, s:, :] = True\n","        attn_mask[-1, :, s:, :, :s, :] = True\n","        attn_mask[:, -1, :, :s, :, s:] = True\n","        attn_mask[:, -1, :, s:, :, :s] = True\n","        attn_mask = rearrange(attn_mask, 'w1 w2 p1 p2 p3 p4 -> 1 1 (w1 w2) (p1 p2) (p3 p4)')\n","        return attn_mask\n","\n","    def forward(self, x):\n","        \"\"\" Forward pass of Window Multi-head Self-attention module.\n","        Args:\n","            x: input tensor with shape of [b h w c];\n","            attn_mask: attention mask, fill -inf where the value is True;\n","        Returns:\n","            output: tensor shape [b h w c]\n","        \"\"\"\n","        if self.type!='W': x = torch.roll(x, shifts=(-(self.window_size//2), -(self.window_size//2)), dims=(1,2))\n","        x = rearrange(x, 'b (w1 p1) (w2 p2) c -> b w1 w2 p1 p2 c', p1=self.window_size, p2=self.window_size)\n","        h_windows = x.size(1)\n","        w_windows = x.size(2)\n","        # sqaure validation\n","        assert h_windows == w_windows\n","\n","        x = rearrange(x, 'b w1 w2 p1 p2 c -> b (w1 w2) (p1 p2) c', p1=self.window_size, p2=self.window_size)\n","        qkv = self.embedding_layer(x)\n","        q, k, v = rearrange(qkv, 'b nw np (threeh c) -> threeh b nw np c', c=self.head_dim).chunk(3, dim=0)\n","        sim = torch.einsum('hbwpc,hbwqc->hbwpq', q, k) * self.scale\n","        # Adding learnable relative embedding\n","        sim = sim + rearrange(self.relative_embedding(), 'h p q -> h 1 1 p q')\n","        # Using Attn Mask to distinguish different subwindows.\n","        if self.type != 'W':\n","            attn_mask = self.generate_mask(h_windows, self.window_size, shift=self.window_size//2)\n","            sim = sim.masked_fill_(attn_mask, float(\"-inf\"))\n","\n","        probs = nn.functional.softmax(sim, dim=-1)\n","        output = torch.einsum('hbwij,hbwjc->hbwic', probs, v)\n","        output = rearrange(output, 'h b w p c -> b w p (h c)')\n","        output = self.linear(output)\n","        output = rearrange(output, 'b (w1 w2) (p1 p2) c -> b (w1 p1) (w2 p2) c', w1=h_windows, p1=self.window_size)\n","\n","        if self.type!='W': output = torch.roll(output, shifts=(self.window_size//2, self.window_size//2), dims=(1,2))\n","        return output\n","\n","    def relative_embedding(self):\n","        cord = torch.tensor(np.array([[i, j] for i in range(self.window_size) for j in range(self.window_size)]))\n","        relation = cord[:, None, :] - cord[None, :, :] + self.window_size -1\n","        # negative is allowed\n","        return self.relative_position_params[:, relation[:,:,0], relation[:,:,1]]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"EWwI-snFG1_n","executionInfo":{"status":"ok","timestamp":1713897812129,"user_tz":-360,"elapsed":5,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, input_dim, output_dim, head_dim, window_size, drop_path, type='W', input_resolution=None):\n","        \"\"\" SwinTransformer Block\n","        \"\"\"\n","        super(Block, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        assert type in ['W', 'SW']\n","        self.type = type\n","        if input_resolution <= window_size:\n","            self.type = 'W'\n","\n","        print(\"Block Initial Type: {}, drop_path_rate:{:.6f}\".format(self.type, drop_path))\n","        self.ln1 = nn.LayerNorm(input_dim)\n","        self.msa = WMSA(input_dim, input_dim, head_dim, window_size, self.type)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.ln2 = nn.LayerNorm(input_dim)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(input_dim, 4 * input_dim),\n","            nn.GELU(),\n","            nn.Linear(4 * input_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = x + self.drop_path(self.msa(self.ln1(x)))\n","        x = x + self.drop_path(self.mlp(self.ln2(x)))\n","        return x"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"cfNgt6LRG1_o","executionInfo":{"status":"ok","timestamp":1713897812995,"user_tz":-360,"elapsed":870,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["class SwinTransformer(nn.Module):\n","    \"\"\" Implementation of Swin Transformer https://arxiv.org/abs/2103.14030\n","    In this Implementation, the standard shape of data is (b h w c), which is a similar protocal as cnn.\n","    \"\"\"\n","    #todo make layers using configs\n","    def __init__(self, num_classes, config=[2,2,6,2], dim=96, drop_path_rate=0.2, input_resolution=224):\n","        super(SwinTransformer, self).__init__()\n","        self.config = config\n","        self.dim = dim\n","        self.head_dim = 32\n","        self.window_size = 7\n","        # self.patch_partition = Rearrange('b c (h1 sub_h) (w1 sub_w) -> b h1 w1 (c sub_h sub_w)', sub_h=4, sub_w=4)\n","\n","        # drop path rate for each layer\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(config))]\n","\n","        begin = 0\n","        self.stage1 = [nn.Conv2d(3, dim, kernel_size=4, stride=4),\n","                       Rearrange('b c h w -> b h w c'),\n","                       nn.LayerNorm(dim),] + \\\n","                      [Block(dim, dim, self.head_dim, self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW', input_resolution//4)\n","                      for i in range(config[0])]\n","        begin += config[0]\n","        self.stage2 = [Rearrange('b (h neih) (w neiw) c -> b h w (neiw neih c)', neih=2, neiw=2),\n","                       nn.LayerNorm(4*dim), nn.Linear(4*dim, 2*dim, bias=False),] + \\\n","                      [Block(2*dim, 2*dim, self.head_dim, self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW', input_resolution//8)\n","                      for i in range(config[1])]\n","        begin += config[1]\n","        self.stage3 = [Rearrange('b (h neih) (w neiw) c -> b h w (neiw neih c)', neih=2, neiw=2),\n","                       nn.LayerNorm(8*dim), nn.Linear(8*dim, 4*dim, bias=False),] + \\\n","                      [Block(4*dim, 4*dim, self.head_dim, self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW',input_resolution//16)\n","                      for i in range(config[2])]\n","        begin += config[2]\n","        self.stage4 = [Rearrange('b (h neih) (w neiw) c -> b h w (neiw neih c)', neih=2, neiw=2),\n","                       nn.LayerNorm(16*dim), nn.Linear(16*dim, 8*dim, bias=False),] + \\\n","                      [Block(8*dim, 8*dim, self.head_dim, self.window_size, dpr[i+begin], 'W' if not i%2 else 'SW', input_resolution//32)\n","                      for i in range(config[3])]\n","\n","        self.stage1 = nn.Sequential(*self.stage1)\n","        self.stage2 = nn.Sequential(*self.stage2)\n","        self.stage3 = nn.Sequential(*self.stage3)\n","        self.stage4 = nn.Sequential(*self.stage4)\n","\n","        self.norm_last = nn.LayerNorm(dim * 8)\n","        self.mean_pool = Reduce('b h w c -> b c', reduction='mean')\n","        self.classifier = nn.Linear(8*dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def forward(self, x):\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","        x = self.norm_last(x)\n","\n","        x = self.mean_pool(x)\n","        x = self.classifier(x)\n","        return x\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"gJne_DPjG1_o","executionInfo":{"status":"ok","timestamp":1713897812995,"user_tz":-360,"elapsed":6,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["def Swin_T(num_classes, config=[2,2,6,2], dim=96, **kwargs):\n","    return SwinTransformer(num_classes, config=config, dim=dim, **kwargs)\n","\n","def Swin_S(num_classes, config=[2,2,18,2], dim=96, **kwargs):\n","    return SwinTransformer(num_classes, config=config, dim=dim, **kwargs)\n","\n","def Swin_B(num_classes, config=[2,2,18,2], dim=128, **kwargs):\n","    return SwinTransformer(num_classes, config=config, dim=dim, **kwargs)\n","\n","def Swin_L(num_classes, config=[2,2,18,2], dim=192, **kwargs):\n","    return SwinTransformer(num_classes, config=config, dim=dim, **kwargs)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1maX3cFfG1_o","executionInfo":{"status":"ok","timestamp":1713897812995,"user_tz":-360,"elapsed":5,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cuda', save_path='best_model.pth'):\n","    train_losses = []\n","    val_losses = []\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        # Training\n","        model.train()\n","        train_loss = 0.0\n","        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n","            images, labels = images.to(device).float(), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * images.size(0)\n","\n","        train_loss /= len(train_loader.dataset)\n","        train_losses.append(train_loss)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device).float(), labels.to(device)\n","\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * images.size(0)\n","\n","            val_loss /= len(val_loader.dataset)\n","            val_losses.append(val_loss)\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                torch.save(model.state_dict(), save_path)\n","\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n","              f'Train Loss: {train_loss:.4f}, '\n","              f'Val Loss: {val_loss:.4f}')\n","\n","    return train_losses, val_losses\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"2Gic3xVJG1_p","executionInfo":{"status":"ok","timestamp":1713897812996,"user_tz":-360,"elapsed":6,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["# Define the accuracy calculation function\n","def calculate_accuracy(model, data_loader, device='cuda'):\n","    correct = 0\n","    total = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    return correct / total"]},{"cell_type":"markdown","metadata":{"id":"o5KXBEwtW4v-"},"source":["# RAF-DB dataset train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7ZEEC3EG1_t"},"outputs":[],"source":["# Define your custom dataset class to load RAF-DB dataset\n","class RAFDBDataset(Dataset):\n","    def __init__(self, root_dir, txt_file, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.data = []  # List to store (image_path, label) pairs\n","        with open(txt_file, 'r') as file:\n","            for line in file.readlines():\n","                image_name, label = line.split()\n","                image_name_list = image_name.split('.')\n","                last_img_name = image_name_list[0] + '_aligned.' + image_name_list[1]\n","                image_path = os.path.join(root_dir, last_img_name)\n","                if os.path.exists(image_path):\n","                    self.data.append((image_path, int(label)))\n","                else:\n","                    continue\n","                    #print(f\"Image not found: {last_img_name}\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        image_path, label = self.data[idx]\n","        image = Image.open(image_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","\n","train_folder = '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/RAF-DB/image/train'\n","test_folder = '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/RAF-DB/image/test'\n","train_txt_file = '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/RAF-DB/image/train.txt'\n","test_txt_file = '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/RAF-DB/image/test.txt'\n","\n","\n","# Define data transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images\n","])\n","\n","# Create RAF-DB datasets for train and validation\n","rafdb_train_dataset = RAFDBDataset(train_folder, train_txt_file, transform=transform)\n","rafdb_val_dataset = RAFDBDataset(test_folder, test_txt_file, transform=transform)\n","\n","batch_size = 32\n","num_epochs = 25\n","learning_rate = 0.01\n","rafdb_train_loader = DataLoader(rafdb_train_dataset, batch_size=batch_size, shuffle=True)\n","rafdb_val_loader = DataLoader(rafdb_val_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"0xt_lA3SXApT","outputId":"994bee24-40ed-4eea-de69-d1b726b3019f","executionInfo":{"status":"ok","timestamp":1713864903303,"user_tz":-360,"elapsed":623113,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Block Initial Type: W, drop_path_rate:0.000000\n","Block Initial Type: SW, drop_path_rate:0.008696\n","Block Initial Type: W, drop_path_rate:0.017391\n","Block Initial Type: SW, drop_path_rate:0.026087\n","Block Initial Type: W, drop_path_rate:0.034783\n","Block Initial Type: SW, drop_path_rate:0.043478\n","Block Initial Type: W, drop_path_rate:0.052174\n","Block Initial Type: SW, drop_path_rate:0.060870\n","Block Initial Type: W, drop_path_rate:0.069565\n","Block Initial Type: SW, drop_path_rate:0.078261\n","Block Initial Type: W, drop_path_rate:0.086957\n","Block Initial Type: SW, drop_path_rate:0.095652\n","Block Initial Type: W, drop_path_rate:0.104348\n","Block Initial Type: SW, drop_path_rate:0.113043\n","Block Initial Type: W, drop_path_rate:0.121739\n","Block Initial Type: SW, drop_path_rate:0.130435\n","Block Initial Type: W, drop_path_rate:0.139130\n","Block Initial Type: SW, drop_path_rate:0.147826\n","Block Initial Type: W, drop_path_rate:0.156522\n","Block Initial Type: SW, drop_path_rate:0.165217\n","Block Initial Type: W, drop_path_rate:0.173913\n","Block Initial Type: SW, drop_path_rate:0.182609\n","Block Initial Type: W, drop_path_rate:0.191304\n","Block Initial Type: W, drop_path_rate:0.200000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [1/25], Train Loss: 3.7762, Val Loss: 2.8682\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [2/25], Train Loss: 1.9493, Val Loss: 1.7233\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [3/25], Train Loss: 1.6383, Val Loss: 1.8982\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [4/25], Train Loss: 1.5315, Val Loss: 1.5256\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [5/25], Train Loss: 1.4738, Val Loss: 1.4502\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [6/25], Train Loss: 1.3662, Val Loss: 1.4272\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [7/25], Train Loss: 1.4075, Val Loss: 1.6161\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [8/25], Train Loss: 1.4087, Val Loss: 1.4558\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [9/25], Train Loss: 1.3679, Val Loss: 1.4257\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [10/25], Train Loss: 1.3896, Val Loss: 1.6180\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [11/25], Train Loss: 1.4028, Val Loss: 1.5753\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [12/25], Train Loss: 1.6284, Val Loss: 1.5066\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [13/25], Train Loss: 1.3941, Val Loss: 1.6366\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [14/25], Train Loss: 1.3923, Val Loss: 1.4354\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [15/25], Train Loss: 1.3272, Val Loss: 1.4346\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [16/25], Train Loss: 1.3243, Val Loss: 1.4310\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [17/25], Train Loss: 1.3596, Val Loss: 1.3849\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [18/25], Train Loss: 1.3342, Val Loss: 1.4043\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [19/25], Train Loss: 1.3133, Val Loss: 1.4027\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [20/25], Train Loss: 1.3548, Val Loss: 1.4861\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [21/25], Train Loss: 1.3622, Val Loss: 1.3596\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [22/25], Train Loss: 1.2985, Val Loss: 1.3826\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [23/25], Train Loss: 1.2809, Val Loss: 1.3635\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [24/25], Train Loss: 1.2938, Val Loss: 1.4384\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch [25/25], Train Loss: 1.3017, Val Loss: 1.4416\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Parameters for the demo\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","save_path = \"/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/rafdb_best_model.pth\"\n","\n","# Define the Swin Transformer model\n","model = Swin_S(num_classes=12000).to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","train_losses, val_losses = train_model(model, rafdb_train_loader, rafdb_val_loader, criterion, optimizer, num_epochs=num_epochs, device=device, save_path=save_path)\n","\n","# Plot the training and validation loss curves\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHGFd8hlfleV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713867127815,"user_tz":-360,"elapsed":3833,"user":{"displayName":"Research73","userId":"16443954781959923532"}},"outputId":"291a6862-5ba4-4650-a2b5-8872329e829e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Block Initial Type: W, drop_path_rate:0.000000\n","Block Initial Type: SW, drop_path_rate:0.008696\n","Block Initial Type: W, drop_path_rate:0.017391\n","Block Initial Type: SW, drop_path_rate:0.026087\n","Block Initial Type: W, drop_path_rate:0.034783\n","Block Initial Type: SW, drop_path_rate:0.043478\n","Block Initial Type: W, drop_path_rate:0.052174\n","Block Initial Type: SW, drop_path_rate:0.060870\n","Block Initial Type: W, drop_path_rate:0.069565\n","Block Initial Type: SW, drop_path_rate:0.078261\n","Block Initial Type: W, drop_path_rate:0.086957\n","Block Initial Type: SW, drop_path_rate:0.095652\n","Block Initial Type: W, drop_path_rate:0.104348\n","Block Initial Type: SW, drop_path_rate:0.113043\n","Block Initial Type: W, drop_path_rate:0.121739\n","Block Initial Type: SW, drop_path_rate:0.130435\n","Block Initial Type: W, drop_path_rate:0.139130\n","Block Initial Type: SW, drop_path_rate:0.147826\n","Block Initial Type: W, drop_path_rate:0.156522\n","Block Initial Type: SW, drop_path_rate:0.165217\n","Block Initial Type: W, drop_path_rate:0.173913\n","Block Initial Type: SW, drop_path_rate:0.182609\n","Block Initial Type: W, drop_path_rate:0.191304\n","Block Initial Type: W, drop_path_rate:0.200000\n","Validation Accuracy: 0.48\n"]}],"source":["# Load the best model weights\n","save_path = \"/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/rafdb_best_model.pth\"\n","best_model = Swin_S(num_classes=12000).to(device)\n","best_model.load_state_dict(torch.load(save_path))\n","\n","# Calculate accuracy\n","val_accuracy = calculate_accuracy(best_model, rafdb_val_loader, device=device)\n","print(f'Validation Accuracy: {val_accuracy}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1hH4F00flh4"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3vxQfIIbFWps"},"source":["#CK+ dataset train"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"UktbvMEfFbar","executionInfo":{"status":"ok","timestamp":1713897859284,"user_tz":-360,"elapsed":1196,"user":{"displayName":"Research73","userId":"16443954781959923532"}}},"outputs":[],"source":["class CKPlusDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, usage='Training'):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.data = self.data[self.data['Usage'] == usage]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        pixels = self.data.iloc[idx]['pixels'].split(' ')\n","        pixels = [float(p) for p in pixels]\n","        pixels = np.array(pixels, dtype=np.uint8).reshape(48, 48, 1)\n","        pixels = np.concatenate([pixels] * 3, axis=2)\n","        emotion = float(self.data.iloc[idx]['emotion'])\n","        pixels = pixels / 255.0\n","        if self.transform:\n","            pixels = self.transform(pixels)\n","        return pixels, emotion\n","\n","train_dataset = CKPlusDataset(csv_file='/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/CK+/ck+.csv', transform=ToTensor(), usage='Training')\n","val_dataset = CKPlusDataset(csv_file='/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/CK+/ck+.csv', transform=ToTensor(), usage='PublicTest')\n","\n","batch_size = 32\n","num_epochs = 25\n","learning_rate = 0.01\n","ck_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","ck_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"AAnNViSyPTRg","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1713902453717,"user_tz":-360,"elapsed":460,"user":{"displayName":"Research73","userId":"16443954781959923532"}},"outputId":"621b6015-a585-4ef0-e88d-9158781c161a"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'torch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6ac7d12a05af>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Parameters for the demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/ck+_best_model.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the Swin Transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["# Parameters for the demo\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","save_path = \"/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/ck+_best_model.pth\"\n","\n","# Define the Swin Transformer model\n","model = Swin_S(num_classes=734).to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","train_losses, val_losses = train_model(model, ck_train_loader, ck_val_loader, criterion, optimizer, num_epochs=num_epochs, device=device, save_path=save_path)\n","\n","# Plot the training and validation loss curves\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBXxcyslG1_s","colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"status":"error","timestamp":1713866524059,"user_tz":-360,"elapsed":857,"user":{"displayName":"Research73","userId":"16443954781959923532"}},"outputId":"ce97a72b-7001-4f50-9b80-945cfb5b9d7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Block Initial Type: W, drop_path_rate:0.000000\n","Block Initial Type: SW, drop_path_rate:0.018182\n","Block Initial Type: W, drop_path_rate:0.036364\n","Block Initial Type: SW, drop_path_rate:0.054545\n","Block Initial Type: W, drop_path_rate:0.072727\n","Block Initial Type: SW, drop_path_rate:0.090909\n","Block Initial Type: W, drop_path_rate:0.109091\n","Block Initial Type: SW, drop_path_rate:0.127273\n","Block Initial Type: W, drop_path_rate:0.145455\n","Block Initial Type: SW, drop_path_rate:0.163636\n","Block Initial Type: W, drop_path_rate:0.181818\n","Block Initial Type: W, drop_path_rate:0.200000\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/ck+_best_model.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-94fdf884f618>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the best model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwin_T\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m920\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/ck+_best_model.pth'"]}],"source":["# Load the best model weights\n","save_path = \"/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/ck+_best_model.pth\"\n","best_model = Swin_T(num_classes=734).to(device)\n","best_model.load_state_dict(torch.load(save_path))\n","\n","# Calculate accuracy\n","val_accuracy = calculate_accuracy(best_model, ck_val_loader, device=device)\n","print(f'Validation Accuracy: {val_accuracy}')"]},{"cell_type":"code","source":[],"metadata":{"id":"nzfeAEVsR845"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZEGfhAsXA_u"},"source":["# FER2013 dataset train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zg2t5AoXO_C"},"outputs":[],"source":["class fer2013Dataset(Dataset):\n","    def __init__(self, csv_file, transform=None, usage='Training'):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.data = self.data[self.data['Usage'] == usage]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        pixels = self.data.iloc[idx]['pixels'].split(' ')\n","        pixels = [float(p) for p in pixels]\n","        pixels = np.array(pixels, dtype=np.uint8).reshape(48, 48, 1)  # Reshape pixels to image size\n","        emotion = float(self.data.iloc[idx]['emotion'])\n","        pixels = pixels / 255.0\n","        if self.transform:\n","            pixels = self.transform(pixels)\n","        return pixels, emotion\n","\n","# Load dataset and split into train/validation sets\n","train_dataset = CKPlusDataset(csv_file='/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/FER2013/fer2013/fer2013 short.csv', transform=ToTensor(), usage='Training')\n","val_dataset = CKPlusDataset(csv_file='/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/Dataset/FER2013/fer2013/fer2013 short.csv', transform=ToTensor(), usage='PublicTest')\n","\n","# Define data loaders\n","batch_size = 32\n","num_epochs = 25\n","learning_rate = 0.01\n","fer_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","fer_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oruh6nWqXPam","colab":{"base_uri":"https://localhost:8080/","height":772},"executionInfo":{"status":"error","timestamp":1713866532735,"user_tz":-360,"elapsed":1250,"user":{"displayName":"Research73","userId":"16443954781959923532"}},"outputId":"24fab8b8-fa88-45f1-ac1d-20061c829c2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Block Initial Type: W, drop_path_rate:0.000000\n","Block Initial Type: SW, drop_path_rate:0.008696\n","Block Initial Type: W, drop_path_rate:0.017391\n","Block Initial Type: SW, drop_path_rate:0.026087\n","Block Initial Type: W, drop_path_rate:0.034783\n","Block Initial Type: SW, drop_path_rate:0.043478\n","Block Initial Type: W, drop_path_rate:0.052174\n","Block Initial Type: SW, drop_path_rate:0.060870\n","Block Initial Type: W, drop_path_rate:0.069565\n","Block Initial Type: SW, drop_path_rate:0.078261\n","Block Initial Type: W, drop_path_rate:0.086957\n","Block Initial Type: SW, drop_path_rate:0.095652\n","Block Initial Type: W, drop_path_rate:0.104348\n","Block Initial Type: SW, drop_path_rate:0.113043\n","Block Initial Type: W, drop_path_rate:0.121739\n","Block Initial Type: SW, drop_path_rate:0.130435\n","Block Initial Type: W, drop_path_rate:0.139130\n","Block Initial Type: SW, drop_path_rate:0.147826\n","Block Initial Type: W, drop_path_rate:0.156522\n","Block Initial Type: SW, drop_path_rate:0.165217\n","Block Initial Type: W, drop_path_rate:0.173913\n","Block Initial Type: SW, drop_path_rate:0.182609\n","Block Initial Type: W, drop_path_rate:0.191304\n","Block Initial Type: W, drop_path_rate:0.200000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"error","ename":"RuntimeError","evalue":"Given groups=1, weight of size [96, 3, 4, 4], expected input[32, 1, 48, 48] to have 3 channels, but got 1 channels instead","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-3ce37e873d0c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfer_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfer_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Plot the training and validation loss curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-057e458dd83c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-f6dd0a584429>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [96, 3, 4, 4], expected input[32, 1, 48, 48] to have 3 channels, but got 1 channels instead"]}],"source":["# Parameters for the demo\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","save_path = \"/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/fer2013_best_model.pth\"\n","\n","# Define the Swin Transformer model\n","model = Swin_S(num_classes=500).to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","train_losses, val_losses = train_model(model, fer_train_loader, fer_val_loader, criterion, optimizer, num_epochs=num_epochs, device=device, save_path=save_path)\n","\n","# Plot the training and validation loss curves\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OkZlJsUXPmN","colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"status":"error","timestamp":1713866536943,"user_tz":-360,"elapsed":829,"user":{"displayName":"Research73","userId":"16443954781959923532"}},"outputId":"22b905f5-21df-4692-f25c-cff872b26f04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Block Initial Type: W, drop_path_rate:0.000000\n","Block Initial Type: SW, drop_path_rate:0.018182\n","Block Initial Type: W, drop_path_rate:0.036364\n","Block Initial Type: SW, drop_path_rate:0.054545\n","Block Initial Type: W, drop_path_rate:0.072727\n","Block Initial Type: SW, drop_path_rate:0.090909\n","Block Initial Type: W, drop_path_rate:0.109091\n","Block Initial Type: SW, drop_path_rate:0.127273\n","Block Initial Type: W, drop_path_rate:0.145455\n","Block Initial Type: SW, drop_path_rate:0.163636\n","Block Initial Type: W, drop_path_rate:0.181818\n","Block Initial Type: W, drop_path_rate:0.200000\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/ck+_best_model.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-8869002f36c0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the best model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwin_T\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m920\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/ck+_best_model.pth'"]}],"source":["# Load the best model weights\n","save_path = \"/content/drive/My Drive/Content Recommendation System based on Facial expression recognition using Swin Transformer and Haar Cascade/swin_transformer_model/fer2013 short_best_model.pth\"\n","best_model = Swin_T(num_classes=500).to(device)\n","best_model.load_state_dict(torch.load(save_path))\n","\n","# Calculate accuracy\n","val_accuracy = calculate_accuracy(best_model, fer_val_loader, device=device)\n","print(f'Validation Accuracy: {val_accuracy}')"]},{"cell_type":"code","source":[],"metadata":{"id":"L9cF5-bEYwtK"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}